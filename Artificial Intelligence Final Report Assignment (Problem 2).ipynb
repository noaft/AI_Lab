{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPai7ZR+elatxW92et0TtXw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"W6HDTSmy8_7f"},"outputs":[],"source":["!pip install torchtext==0.17.0\n","!pip install portalocker\n","\n","import torch\n","import torch.nn.functional as F\n","import torchtext\n","import torchtext.datasets\n","import torch.nn as nn"]},{"cell_type":"code","source":["train_iter, test_iter = torchtext.datasets.IMDB(split=('train','test')) #split train and test dataset\n","tokenizer = torchtext.data.utils.get_tokenizer('basic_english') #use basic english tokenizer"],"metadata":{"id":"Cdb4Ra8-9Ewn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["MODELNAME = \"imdb-lstm.model\"\n","EPOCH = 10\n","BATCHSIZE = 64\n","LR = 5e-5\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print (DEVICE)"],"metadata":{"id":"a2tvCZrm9GJQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data = [(label, tokenizer (line)) for label, line in train_iter]\n","train_data.sort(key = lambda x: len(x[1]))\n","test_data = [(label, tokenizer (line)) for label, line in test_iter]\n","test_data.sort(key = lambda x: len(x[1]))"],"metadata":{"id":"hbXXSIoI9Hj2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(10):\n","  print(train_data[i])"],"metadata":{"id":"QmlD-gx_9JHG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#use same bow to embedding\n","def make_vocab(train_data, min_freq):\n","  vocab= {}\n","  for label, tokenlist in train_data:\n","    for token in tokenlist:\n","      if token not in vocab:\n","        vocab [token] = 0\n","      vocab [token] += 1\n","  vocablist = [('<unk>', 0), ('<pad>', 0), ('<cls>', 0), ('<eos>',0)]\n","  vocabidx = {}\n","  #example: \"Ton\" : 5 -> token = \"Ton\" and freq = 5\n","  for token, freq in vocab.items():\n","    if freq >= min_freq:\n","      idx= len(vocablist)\n","      vocablist.append((token, freq))\n","      vocabidx [token] = idx\n","  vocabidx[ '<unk>']=0\n","  vocabidx['<pad>']=1\n","  vocabidx['<cls>']=2\n","  vocabidx['<eos>']=3\n","  return vocablist, vocabidx"],"metadata":{"id":"hP1u-eSZ9Kav"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocablist, vocabidx = make_vocab(train_data, 10)"],"metadata":{"id":"v_5UpsWg9L-o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocess (data, vocabidx):\n","  rr = []\n","  for label, tokenlist in data:\n","    tkl = ['<cls>']\n","    for token in tokenlist:\n","      tkl.append(token if token in vocabidx else '<unk>')\n","    tkl.append('<eos>')\n","    rr.append((label, tkl))\n","  return rr"],"metadata":{"id":"lSlLHCbG9NrH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data = preprocess (train_data, vocabidx)\n","test_data = preprocess (test_data, vocabidx)\n","for i in range(10):\n","  print(train_data[i])"],"metadata":{"id":"8JkqSqY99PIH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def make_batch(data, batchsize):\n","  bb = []\n","  blabel = []\n","  btokenlist = []\n","  for label, tokenlist in data:\n","    blabel.append(label)\n","    btokenlist.append(tokenlist)\n","    if len(blabel) > batchsize:\n","      bb.append((btokenlist, blabel))\n","      blabel = []\n","      btokenlist = []\n","  if len(blabel) > 0:\n","    bb.append((btokenlist, blabel))\n","  return bb\n","\n","train_data = make_batch(train_data, BATCHSIZE)\n","test_data = make_batch(test_data, BATCHSIZE)\n","for i in range(10):\n","  print(train_data[i])"],"metadata":{"id":"GfMRJ42Y9QeY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#create padding for same len\n","def padding(bb):\n","  for tokenlists, labels in bb:\n","    maxlen = max([len(x) for x in tokenlists])\n","    for tkl in tokenlists:\n","      for i in range(maxlen-len(tkl)):\n","        tkl.append('<pad>')\n","  return bb"],"metadata":{"id":"gylVU5Yp9R_Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data = padding(train_data)\n","test_data = padding(test_data)\n","for i in range(10):\n","  print(train_data[i])"],"metadata":{"id":"S4fJ6a4h9TZv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#embedding data\n","def word2id(bb, vocabidx):\n","  rr = []\n","  for tokenlists, labels in bb:\n","    id_labels = [label - 1 for label in labels]\n","    id_tokenlists = []\n","    for tokenlist in tokenlists:\n","      id_tokenlists.append([vocabidx[token] for token in tokenlist])\n","    rr.append((id_tokenlists, id_labels))\n","  return rr"],"metadata":{"id":"HU4CFq8s9U7P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data = word2id (train_data, vocabidx)\n","test_data = word2id(test_data, vocabidx)\n","for i in range(10):\n","  print(train_data[i])"],"metadata":{"id":"z0VibEWH9WQH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MyLSTM(nn.Module):\n","    def __init__(self):\n","        super(MyLSTM, self).__init__()\n","        vocabsize = len(vocablist)\n","        self.emb = nn.Embedding(vocabsize, 300, padding_idx=vocabidx['<pad>'])\n","        #create auto hidden size\n","        self.lstm = nn.LSTM(input_size=300, hidden_size=300, num_layers=1, batch_first=True)\n","        self.fc = nn.Linear(300, 2)\n","\n","    def forward(self, x):\n","        e = self.emb(x)\n","        h0 = torch.zeros(1, x.size(0), 300).to(DEVICE)\n","        c0 = torch.zeros(1, x.size(0), 300).to(DEVICE)\n","        out, _ = self.lstm(e, (h0, c0))\n","        out = out[:, -1, :]  # Lấy đầu ra của LSTM ở thời điểm cuối cùng\n","        out = self.fc(out)\n","        return out"],"metadata":{"id":"R15-AQ0R9XsI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train():\n","  model = MyLSTM().to (DEVICE)\n","  optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n","  for epoch in range(EPOCH):\n","    loss = 0\n","    for tokenlists, labels in train_data:\n","      tokenlists = torch.tensor(tokenlists, dtype = torch.int64).to (DEVICE)\n","      labels = torch.tensor(labels, dtype=torch.int64).to (DEVICE)\n","      optimizer.zero_grad()\n","      y = model(tokenlists)\n","      batchloss = F.cross_entropy (y, labels)\n","      batchloss.backward()\n","      optimizer.step()\n","      loss = loss + batchloss.item()\n","    print(\"epoch\", epoch,\": loss\", loss)\n","  torch.save(model.state_dict(), MODELNAME)"],"metadata":{"id":"HFek3ars9ZNf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def test():\n","  total=0\n","  correct = 0\n","  model = MyLSTM().to (DEVICE)\n","  model.load_state_dict(torch. load (MODELNAME))\n","  model.eval()\n","  for tokenlists, labels in test_data:\n","    total += len(labels)\n","    tokenlists = torch.tensor (tokenlists, dtype = torch.int64).to (DEVICE)\n","    labels = torch.tensor (labels, dtype=torch.int64).to (DEVICE)\n","    y=model (tokenlists)\n","    pred_labels = y.max(dim=1) [1]\n","    correct+=(pred_labels == labels).sum()\n","  print(\"correct: \", correct.item())\n","  print(\"total: \",total)\n","  print(\"accuracy: \", (correct.item()/float(total)))"],"metadata":{"id":"HHE7tE8P9a2f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train()"],"metadata":{"id":"ImUMFPOF9cYP"},"execution_count":null,"outputs":[]}]}